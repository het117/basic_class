{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6dQ3QpUenVu"
   },
   "source": [
    "# Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaU1M6TFepHk",
    "outputId": "29b606f9-a586-4124-ebce-ab76f9e8ef81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=16pB7xg-XYJAtL6g4BqNy-ervrKnEnnz0\n",
      "To: /content/images.zip\n",
      "100% 7.75M/7.75M [00:00<00:00, 25.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 16pB7xg-XYJAtL6g4BqNy-ervrKnEnnz0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LTrrBKjmXo8",
    "outputId": "65aa0a43-d075-4969-da70-2f88d5b739db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/images.zip\n",
      "   creating: ./images/output/\n",
      "  inflating: ./images/content/content.jpg  \n",
      "  inflating: ./images/style/style.jpg  \n"
     ]
    }
   ],
   "source": [
    "!unzip /content/images.zip -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HZuitSJewXY",
    "outputId": "9b6e977f-ea94-4332-aeb8-3501f1351f60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'detectron2'...\n",
      "remote: Enumerating objects: 15806, done.\u001b[K\n",
      "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
      "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
      "remote: Total 15806 (delta 22), reused 38 (delta 9), pack-reused 15743 (from 1)\u001b[K\n",
      "Receiving objects: 100% (15806/15806), 6.38 MiB | 5.63 MiB/s, done.\n",
      "Resolving deltas: 100% (11516/11516), done.\n",
      "Ignoring dataclasses: markers 'python_version < \"3.7\"' don't match your environment\n",
      "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
      "Collecting yacs>=0.1.8\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
      "Collecting fvcore<0.1.6,>=0.1.5\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting iopath<0.1.10,>=0.1.7\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
      "Collecting omegaconf<2.4,>=2.1\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting hydra-core>=1.1\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting black\n",
      "  Downloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs>=0.1.8) (6.0.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.67.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (4.25.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n",
      "Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.4,>=2.1)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
      "Collecting mypy-extensions>=0.4.3 (from black)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.3.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=581ddb3d8075276a49a4496487b49446150b3b1483a5e5113034cdc752be0ce7\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=cd0761f506e2e7e79d3c86306f3b4c1fc08f30c4f2709768df99f5fe31bdab6c\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "Successfully built fvcore antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 black-24.10.0 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-3.0.0 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "import sys, os, distutils.core\n",
    "\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuA5JCNVenVx"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m6TgQmysenVy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from cycler import cycler\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from cv2 import imread, imwrite\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HFfpuAX0enVz"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NeyeMG6enV0"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rw3i2xFKenV1"
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "def preprocess(img, size=512):\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Resize(size),\n",
    "        T.Normalize(mean=IMAGENET_MEAN.tolist(),\n",
    "                    std=IMAGENET_STD.tolist()),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img).to(dtype=dtype, device=device)\n",
    "\n",
    "def deprocess(img):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda x: x[0]),\n",
    "        T.Normalize(mean=[0, 0, 0], std=[1.0 / s for s in IMAGENET_STD.tolist()]),\n",
    "        T.Normalize(mean=[-m for m in IMAGENET_MEAN.tolist()], std=[1, 1, 1]),\n",
    "        T.Lambda(clamp),\n",
    "        T.Lambda(to_np_uint8),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def clamp(x):\n",
    "    return x.data.clamp_(0.0, 1.0)\n",
    "\n",
    "def to_np_uint8(x):\n",
    "    x = np.round(x.cpu().detach().numpy() * 255.0).astype('uint8')\n",
    "    x = np.transpose(x, [1, 2, 0])\n",
    "    return x\n",
    "\n",
    "def rescale(x):\n",
    "    low, high = x.min(), x.max()\n",
    "    x_rescaled = (x - low) / (high - low)\n",
    "    return x_rescaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAf2E3nuenV1"
   },
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0jPVXexenV2",
    "outputId": "3ba61b0f-03b5-458c-f623-b71a33e02109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current setting for torch: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float\n",
    "\n",
    "print('Current setting for torch:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8BcBWZtmenV3",
    "outputId": "7dbfeabe-cd62-4c92-aa53-fb6f8ba282dc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.5.1+cu121'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GpHpH4CenV3"
   },
   "source": [
    "## load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBj0jxyLenV3",
    "outputId": "89484239-a798-4e0b-9932-d320d7b403b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:07<00:00, 75.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'features.31', 'features.32', 'features.33', 'features.34', 'features.35', 'features.36', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (17): ReLU(inplace=True)\n",
      "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (24): ReLU(inplace=True)\n",
      "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): ReLU(inplace=True)\n",
      "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (31): ReLU(inplace=True)\n",
      "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (33): ReLU(inplace=True)\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): ReLU(inplace=True)\n",
      "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "There are the 37 intermediate [features] layers.\n"
     ]
    }
   ],
   "source": [
    "backbone = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "train_nodes, eval_nodes = get_graph_node_names(backbone)\n",
    "\n",
    "# https://pytorch.org/vision/stable/models.html\n",
    "from pprint import pprint\n",
    "print(eval_nodes)\n",
    "print(backbone.features)\n",
    "\n",
    "return_nodes = []\n",
    "target_name = 'features'\n",
    "\n",
    "for node in eval_nodes:\n",
    "    if target_name in node:\n",
    "        return_nodes.append(node)\n",
    "\n",
    "print(f'There are the {len(return_nodes)} intermediate [{target_name}] layers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "y7JVv80VenV4"
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained model.\n",
    "backbone = create_feature_extractor(backbone, return_nodes=return_nodes)\n",
    "backbone = backbone.to(device=device, dtype=dtype)\n",
    "\n",
    "# turn off the computational graph for the gradidne to save calculation\n",
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def extract_features(x, backbone):\n",
    "    x = x.to(device=device, dtype=dtype)\n",
    "    return backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "12zETKF2enV4"
   },
   "outputs": [],
   "source": [
    "node_name_to_idx = {n: i for i, n in enumerate(return_nodes)}\n",
    "node_idx_to_name = {i: n for i, n in enumerate(return_nodes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nqrDui8enV5"
   },
   "source": [
    "## load Panoptic segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wzmwS5a5enV5"
   },
   "outputs": [],
   "source": [
    "def conduct_segmentation(style_image):\n",
    "    # Inference with a panoptic segmentation model\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
    "    segmentator = DefaultPredictor(cfg)\n",
    "\n",
    "    im = imread(style_image)\n",
    "    panoptic_seg, segments_info = segmentator(im)[\"panoptic_seg\"]\n",
    "\n",
    "    for seg_in in segments_info:\n",
    "        if seg_in['isthing']:\n",
    "            seg_in['class_name'] = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes[seg_in['category_id']]\n",
    "        else:\n",
    "            seg_in['class_name'] = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).stuff_classes[seg_in['category_id']]\n",
    "\n",
    "    return {'seg_out': panoptic_seg.to(\"cpu\"), 'seg_info': segments_info}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wJuAWxUenV6"
   },
   "source": [
    "## Computing Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOB6OCzkenV7"
   },
   "source": [
    "### Content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mpkY_WeXenV7"
   },
   "outputs": [],
   "source": [
    "def content_loss(feats, content_layer_indices, content_targets, content_weights):\n",
    "    loss = torch.Tensor([0.0]).to(device=device, dtype=dtype)\n",
    "\n",
    "    for (layer_id, target, weight) in zip(content_layer_indices, content_targets, content_weights):\n",
    "        loss += weight * torch.sum((feats[node_idx_to_name[layer_id]] - target)**2) / feats[node_idx_to_name[layer_id]].numel()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYmCNMBHenV7"
   },
   "source": [
    "### Style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "n2War152enV7"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(features, normalize=True):\n",
    "    N, C, H, W = features.shape\n",
    "    feat = features.transpose(0, 1).reshape(C, -1)\n",
    "    gram = feat.mm(feat.t())\n",
    "\n",
    "    if normalize == True:\n",
    "        gram /= N * C * H * W\n",
    "\n",
    "    return gram\n",
    "\n",
    "def gram_matrix_weighted(features, weights, normalize=True):\n",
    "    N, C, H, W = features.shape\n",
    "\n",
    "    wei = weights.repeat(N, C, 1, 1).to(dtype=torch.float32)\n",
    "    features = features * wei\n",
    "\n",
    "    feat = features.transpose(0, 1).reshape(C, -1)\n",
    "    gram = feat.mm(feat.t())\n",
    "\n",
    "    if normalize == True:\n",
    "        gram /= (wei ** 2).sum()\n",
    "\n",
    "    return gram\n",
    "\n",
    "def style_loss(feats, style_layer_indices, style_targets, style_weights):\n",
    "    loss = torch.Tensor([0.0]).to(device=device, dtype=dtype)\n",
    "\n",
    "    for (layer_id, target_gram, weight) in zip(style_layer_indices, style_targets, style_weights):\n",
    "        gram = gram_matrix(feats[node_idx_to_name[layer_id]])\n",
    "        loss += weight * torch.sum((target_gram - gram)**2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yf_CBRkenV7"
   },
   "source": [
    "### Total-variation regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qU_I2FOzenV8"
   },
   "outputs": [],
   "source": [
    "def tv_loss(img, tv_weight):\n",
    "    h_filter = torch.Tensor([\n",
    "        [[[1], [-1]], [[0], [ 0]], [[0], [ 0]]],\n",
    "        [[[0], [ 0]], [[1], [-1]], [[0], [ 0]]],\n",
    "        [[[0], [ 0]], [[0], [ 0]], [[1], [-1]]],\n",
    "    ]).to(device=device, dtype=dtype)\n",
    "    h_loss = torch.mean(torch.nn.functional.conv2d(img, h_filter, None, stride=1, padding=0)**2)\n",
    "\n",
    "    w_filter = h_filter.transpose(2, 3)\n",
    "    w_loss = torch.mean(torch.nn.functional.conv2d(img, w_filter, None, stride=1, padding=0)**2)\n",
    "    return tv_weight * (h_loss + w_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9UOtO8TenV8"
   },
   "source": [
    "## Style transfer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UzABOkjTenV8"
   },
   "outputs": [],
   "source": [
    "def style_transfer(content_image, style_image,\n",
    "                   image_size, style_size,\n",
    "                   content_layer_indices, content_weights,\n",
    "                   style_layer_indices, style_weights,\n",
    "                   tv_weight, init='random', start_lr=3.0, clamp_every=2700,\n",
    "                   n_iters=5000, decay_every=900, decay_ratio=0.3, print_every=1000, save_folder=None, visualize=True):\n",
    "    # Extract features for the content image\n",
    "    content_img = imread(content_image)[:, :, ::-1].copy()\n",
    "\n",
    "    content_img = preprocess(content_img, size=image_size)\n",
    "    feats = extract_features(content_img, backbone)\n",
    "    content_targets = []\n",
    "    for idx in content_layer_indices:\n",
    "        content_targets.append(feats[node_idx_to_name[idx]].clone())\n",
    "\n",
    "    # Extract features for the style image\n",
    "    style_img = imread(style_image)[:, :, ::-1].copy()\n",
    "    style_img = preprocess(style_img, size=style_size)\n",
    "    feats = extract_features(style_img, backbone)\n",
    "\n",
    "    # Exploit panoptic segmentation result\n",
    "    panoptic_out = conduct_segmentation(style_image)\n",
    "    seg_out = panoptic_out['seg_out'].reshape(1, 1, *panoptic_out['seg_out'].shape)\n",
    "    weights = torch.ones_like(seg_out).to(dtype=torch.float32)\n",
    "\n",
    "    for seg_info in panoptic_out['seg_info']:\n",
    "        if seg_info['class_name'] in ['person', 'sky', 'horse', 'backpack', 'umbrella', ]:\n",
    "            weights[seg_out == seg_info['id']] = 0\n",
    "    weights[seg_out == 0] = 0.3\n",
    "\n",
    "    if visualize:\n",
    "        f, axarr = plt.subplots(1, 1, figsize=(5.0, 5.0), tight_layout=True)\n",
    "        axarr.axis('off')\n",
    "        axarr.imshow((weights.reshape(weights.shape[2], weights.shape[3], 1).repeat(1, 1, 3) * 255).to(dtype=torch.int32).cpu().numpy())\n",
    "        plt.show()\n",
    "\n",
    "    style_targets = []\n",
    "    for idx in style_layer_indices:\n",
    "        _, _, H, W = feats[node_idx_to_name[idx]].shape\n",
    "        resize = torchvision.transforms.Resize((H, W), torchvision.transforms.InterpolationMode.NEAREST)\n",
    "        style_targets.append(gram_matrix_weighted(feats[node_idx_to_name[idx]].clone(), resize(weights.clone()).to(device)))\n",
    "        # style_targets.append(gram_matrix(feats[node_idx_to_name[idx]].clone()))\n",
    "\n",
    "    # Initialize output image to content image or nois\n",
    "    if init == 'random':\n",
    "        img = torch.Tensor(content_img.size()).uniform_(0, 1)\n",
    "    elif init in ['content', 'contents']:\n",
    "        img = content_img.clone()\n",
    "    else:\n",
    "        raise ValueError(\"style_transfer(init) takes as input among ['random', 'content']\")\n",
    "\n",
    "    img = img.to(dtype=dtype, device=device)\n",
    "\n",
    "    # Turn on the computation graph for gradient calculation\n",
    "    img.requires_grad_()\n",
    "\n",
    "    # Note that we are optimizing the pixel values of the image by passing\n",
    "    # in the img Torch tensor, whose requires_grad flag is set to True\n",
    "    optimizer = torch.optim.Adam([img], lr=start_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, decay_every, decay_ratio)\n",
    "\n",
    "    if visualize:\n",
    "        f, axarr = plt.subplots(1, 2, figsize=(10.0, 5.0), tight_layout=True)\n",
    "        axarr[0].axis('off')\n",
    "        axarr[1].axis('off')\n",
    "        axarr[0].set_title('Content Source Img.')\n",
    "        axarr[1].set_title('Style Source Img.')\n",
    "        axarr[0].imshow(deprocess(content_img))\n",
    "        axarr[1].imshow(deprocess(style_img))\n",
    "        plt.show()\n",
    "\n",
    "    for t in range(n_iters):\n",
    "        # clean graph\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute loss\n",
    "        feats = extract_features(img, backbone)\n",
    "        c_loss = content_loss(feats, content_layer_indices, content_targets, content_weights)\n",
    "        s_loss = style_loss(feats, style_layer_indices, style_targets, style_weights)\n",
    "        t_loss = tv_loss(img, tv_weight)\n",
    "        loss = c_loss + s_loss + t_loss\n",
    "\n",
    "        # Perform gradient descents on our image values\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # display\n",
    "        if t % print_every == 0:\n",
    "            img_print = deprocess(img)\n",
    "\n",
    "            if visualize:\n",
    "                f, axarr = plt.subplots(1, 1, figsize=(5.0, 5.0), tight_layout=True)\n",
    "                print('Iteration {}'.format(t + 1))\n",
    "                axarr.axis('off')\n",
    "                axarr.imshow(img_print)\n",
    "                plt.show()\n",
    "\n",
    "            if save_folder:\n",
    "                imwrite(os.path.join(save_folder, f'i{t:05d}.jpg'), img_print[:, :, ::-1].copy())\n",
    "\n",
    "        # clip too large values\n",
    "        if (t) % round(clamp_every):\n",
    "            img.data.clamp_(-5.0, 5.0)\n",
    "\n",
    "    img_print = deprocess(img)\n",
    "\n",
    "    if visualize:\n",
    "        f, axarr = plt.subplots(1, 1, figsize=(5.0, 5.0), tight_layout=True)\n",
    "        axarr.axis('off')\n",
    "        axarr.imshow(img_print)\n",
    "        plt.show()\n",
    "\n",
    "    if save_folder:\n",
    "        imwrite(os.path.join(save_folder, f'{t:05d}.jpg'), img_print[:, :, ::-1].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM1AR9EzenV8"
   },
   "source": [
    "- `style_transfer()` 함수의 입력 인자 설명\n",
    "\n",
    "| params                | 설명                                                                         |   |   |   |\n",
    "|-----------------------|------------------------------------------------------------------------------|---|---|---|\n",
    "| content_image         | Content image 파일 경로 및 이름                                              |   |   |   |\n",
    "| style_image           | Style image 파일 경로 및 이름                                                |   |   |   |\n",
    "| image_size            | 생성할 Image size                                                            |   |   |   |\n",
    "| style_size            | Style을 계산할 Image size (Style에서 가져오고 싶은 주요 Pattern 크기에 영향) |   |   |   |\n",
    "| content_layer_indices | Content loss를 적용할 계층 번호 [0, len(return_nodes) - 1] 범위의 정수       |   |   |   |\n",
    "| content_weights       | content_layer_indices에 언급한 계층별로 적용할 Weight 강도                   |   |   |   |\n",
    "| style_layer_indices   | Style loss를 적용할 계층 번호 [0, len(return_nodes) - 1] 범위의 정수         |   |   |   |\n",
    "| style_weights         | style_layer_indices에 언급한 계층별로 적용할 Weight 강도                     |   |   |   |\n",
    "| tv_weight             | Total variation 강도                                                         |   |   |   |\n",
    "| init                  | 초기화 방법 (random 또는 content)                                            |   |   |   |\n",
    "| start_lr              | 초기 Learning rate                                                           |   |   |   |\n",
    "| n_iters               | 반복 횟수                                                                    |   |   |   |\n",
    "| decay_every           | Learning rate 감소 주기                                                      |   |   |   |\n",
    "| decay_ratio           | Learning rate 감소율                                                         |   |   |   |\n",
    "| clamp_every           | 영상 범위 제한 적용 주기 (픽셀 표현 범위를 벗어나는 값 억제)                 |   |   |   |\n",
    "| print_every           | 출력 주기                                                                    |   |   |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScPTkm5venV9"
   },
   "source": [
    "## Run neural style transfer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vt-8bZ7BenV9",
    "outputId": "efbace01-91f0-4416-acbc-ef3cba4e5765"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/18 14:09:35 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "model_final_cafdb1.pkl: 0.00B [00:00, ?B/s]\u001b[A\n",
      "model_final_cafdb1.pkl:   0%|          | 8.19k/261M [00:00<1:55:20, 37.7kB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:   0%|          | 647k/261M [00:00<01:42, 2.53MB/s]   \u001b[A\n",
      "model_final_cafdb1.pkl:   3%|▎         | 8.64M/261M [00:00<00:08, 30.1MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:   9%|▉         | 24.0M/261M [00:00<00:03, 71.8MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  18%|█▊        | 47.2M/261M [00:00<00:02, 105MB/s] \u001b[A\n",
      "model_final_cafdb1.pkl:  24%|██▍       | 62.9M/261M [00:00<00:01, 117MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  29%|██▊       | 74.5M/261M [00:00<00:01, 116MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  32%|███▏      | 84.1M/261M [00:00<00:01, 111MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  36%|███▌      | 94.4M/261M [00:01<00:01, 108MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  40%|███▉      | 104M/261M [00:01<00:01, 101MB/s] \u001b[A\n",
      "model_final_cafdb1.pkl:  44%|████▎     | 114M/261M [00:01<00:01, 102MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  48%|████▊     | 126M/261M [00:01<00:01, 106MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  52%|█████▏    | 136M/261M [00:01<00:01, 104MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  56%|█████▌    | 146M/261M [00:01<00:01, 103MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  60%|█████▉    | 156M/261M [00:01<00:01, 102MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  63%|██████▎   | 165M/261M [00:01<00:00, 100MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  66%|██████▋   | 173M/261M [00:01<00:01, 72.8MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  72%|███████▏  | 189M/261M [00:02<00:00, 74.9MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  78%|███████▊  | 204M/261M [00:02<00:00, 85.9MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  78%|███████▊  | 204M/261M [00:02<00:00, 66.5MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  84%|████████▍ | 220M/261M [00:02<00:00, 81.0MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  84%|████████▍ | 220M/261M [00:02<00:00, 59.3MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  91%|█████████ | 236M/261M [00:02<00:00, 67.9MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl:  97%|█████████▋| 252M/261M [00:03<00:00, 79.1MB/s]\u001b[A\n",
      "model_final_cafdb1.pkl: 261MB [00:03, 82.4MB/s]                           \n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 1/1 [03:49<00:00, 229.98s/it]\n"
     ]
    }
   ],
   "source": [
    "stating_index = 0  # TODO: Modify this one if wanting resume the creating\n",
    "\n",
    "content_img_cycler = cycler(content_image=glob.glob(r'/content/images/content/*.jpg'))\n",
    "style_img_cycler = cycler(style_image=glob.glob(r'/content/images/style/*.jpg'))\n",
    "style_cycler = cycler(style_weights=[(1e+3, 1e+3, 1e+3, 1e+3, 1e+3), ])\n",
    "content_cycler = cycler(content_weights=[(3e+2, 3e+2, 0, 0, 0)])\n",
    "                                        #  (1e+3, 1e+3, 0, 0, 0),\n",
    "                                        #  (3e+3, 3e+3, 0, 0, 0),\n",
    "                                        #  (0, 3e+2, 3e+2, 0, 0),\n",
    "                                        #  (0, 1e+3, 1e+3, 0, 0),\n",
    "                                        #  (0, 3e+3, 3e+3, 0, 0)])\n",
    "tv_cycler = cycler(tv_weight=[1e+3])\n",
    "lr_cycler = cycler(start_lr=[5.0])\n",
    "\n",
    "\n",
    "total_cycler = content_img_cycler * style_img_cycler * style_cycler * content_cycler * tv_cycler * lr_cycler\n",
    "\n",
    "for i, c in tqdm(enumerate(total_cycler), total=len(total_cycler)):\n",
    "    if  i < stating_index:\n",
    "        continue\n",
    "\n",
    "    params = {\n",
    "        'image_size' : 192,\n",
    "        'style_size' : 192,\n",
    "        'content_layer_indices' : (0, 5, 10, 14, 19),     # conv_1, 3, 5 7, 9\n",
    "        'style_layer_indices' : (0, 5, 10, 14, 19),       # conv_1, 3, 5 7, 9\n",
    "        'init': 'content',\n",
    "        'n_iters': 10000,\n",
    "        'decay_every': 2300,\n",
    "        'decay_ratio': 0.3,\n",
    "        'clamp_every': 6800,\n",
    "        'print_every': 2000,\n",
    "        **c\n",
    "    }\n",
    "    save_folder_base = r'/content/images/output'\n",
    "    folder_num = 1\n",
    "    while True:\n",
    "        save_folder = os.path.join(save_folder_base, f'{folder_num:05d}')\n",
    "        if os.path.exists(save_folder):\n",
    "            folder_num += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    with open(os.path.join(save_folder, 'setting.json'), 'w') as fp:\n",
    "        json.dump({'index': i, **params}, fp)\n",
    "\n",
    "    style_transfer(**params, save_folder=save_folder, visualize=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3467e056ceb3d9aac642cbda633f9e494892085e32ee6b1fdbedc65e0c72ef11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
