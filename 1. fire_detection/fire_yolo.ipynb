{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOGGTzx7Njvw",
    "outputId": "b8197e15-e8d5-41e5-842c-ee83e5b2e478"
   },
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1Kj47wSBY-9k6Kzfy8r-zlT1H-vUaxgp8\n",
    "!gdown https://drive.google.com/uc?id=1bQ3qw-5O06STYcpoOruKWEANJt2qYRCa\n",
    "!gdown https://drive.google.com/uc?id=1Nh1XgETEAHVjxgpuk6qqxE72MYtZFmqC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbGvyREn5q7J",
    "outputId": "e041d9c6-0099-4fbd-a86d-450657895b90"
   },
   "outputs": [],
   "source": [
    "!unzip /root/Project/class/fire_detection/fire_yolo.zip -d dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "def plot_images_with_boxes(image_paths, label_paths):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(10, 10))\n",
    "    axes = axes.ravel()  # 2x3 그리드를 1차원 배열로 변환\n",
    "\n",
    "    for idx, (image_path, label_path) in enumerate(zip(image_paths, label_paths)):\n",
    "        # 이미지 읽기\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # OpenCV는 BGR 포맷을 사용하므로 RGB로 변환\n",
    "\n",
    "        # 이미지 크기 얻기\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        # 라벨 읽기\n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = f.readlines()\n",
    "\n",
    "        # 이미지 플롯 설정\n",
    "        ax = axes[idx]\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')  # 축 숨기기\n",
    "\n",
    "        for label in labels:\n",
    "            parts = label.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center = float(parts[1]) * width\n",
    "            y_center = float(parts[2]) * height\n",
    "            bbox_width = float(parts[3]) * width\n",
    "            bbox_height = float(parts[4]) * height\n",
    "\n",
    "            # 좌상단 좌표로 변환\n",
    "            x_min = x_center - bbox_width / 2\n",
    "            y_min = y_center - bbox_height / 2\n",
    "\n",
    "            # 박스 그리기\n",
    "            rect = patches.Rectangle((x_min, y_min), bbox_width, bbox_height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_random_image_label_paths(image_folder, label_folder, num_samples=6):\n",
    "    all_images = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n",
    "    sampled_images = random.sample(all_images, num_samples)\n",
    "    \n",
    "    image_paths = [os.path.join(image_folder, img) for img in sampled_images]\n",
    "    label_paths = [os.path.join(label_folder, os.path.splitext(img)[0] + '.txt') for img in sampled_images]\n",
    "\n",
    "    return image_paths, label_paths\n",
    "\n",
    "# 이미지와 라벨 파일이 있는 폴더 경로\n",
    "image_folder = '/root/Project/class/fire_detection/dataset/fire/train/images'\n",
    "label_folder = '/root/Project/class/fire_detection/dataset/fire/train/labels'\n",
    "\n",
    "# 랜덤으로 이미지와 라벨 파일 경로 추출\n",
    "image_paths, label_paths = get_random_image_label_paths(image_folder, label_folder)\n",
    "\n",
    "# 함수 호출\n",
    "plot_images_with_boxes(image_paths, label_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11x.pt\") \n",
    "# pretrained 모델명 기재\n",
    "# yolov8은 yolov8n.pt, yolov8n.pt, yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt\n",
    "# 모델을 프리트레인으로 제공 오른쪽으로 갈수록 무거운 모델\n",
    "\n",
    "model.train(\n",
    "    data=\"fire_data.yaml\",\n",
    "    project=\"Yolov11\",\n",
    "    name=\"firedetection\",\n",
    "    epochs=100, \n",
    "    imgsz=640, \n",
    "    batch=8,\n",
    "    )\n",
    "# 기타 파라메터는 https://docs.ultralytics.com/ko/ 여기서 확인\n",
    "# 파라메터 설정하지 않은 값은 default로 실행됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=Yolov11/firedetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"/root/Project/class/fire_detection/Yolov11/firedetection4/weights/best.pt\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"/root/Project/class/fire_detection/test_video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('/root/Project/class/fire_detection/annotated_video.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame, conf=0.5, iou=0.8)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"/root/Project/class/fire_detection/Yolov11/firedetection4/weights/best.pt\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"/root/Project/class/fire_detection/test_video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('/root/Project/class/fire_detection/annotated_video_h.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "# Define a function for color histogram analysis\n",
    "def is_fire_like(frame, threshold=0.008):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the red color range\n",
    "    lower_red1 = np.array([0, 70, 50])\n",
    "    upper_red1 = np.array([10, 255, 255])\n",
    "    lower_red2 = np.array([170, 70, 50])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "\n",
    "    mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "    red_mask = mask1 | mask2\n",
    "\n",
    "    # Apply the red mask to get red areas\n",
    "    red_only = cv2.bitwise_and(frame, frame, mask=red_mask)\n",
    "\n",
    "    # Calculate histogram and normalize\n",
    "    hist = cv2.calcHist([red_only], [0], red_mask, [256], [0, 256])\n",
    "    hist = hist / hist.sum()\n",
    "\n",
    "    # Measure the irregularity in the red distribution\n",
    "    irregularity = np.std(hist)\n",
    "\n",
    "    return irregularity > threshold\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Process each detection result\n",
    "        for result in results[0].boxes:\n",
    "            # Extract the bounding box coordinates\n",
    "            x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
    "            cropped_frame = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # Apply color histogram analysis to filter out false positives\n",
    "            if is_fire_like(cropped_frame):\n",
    "                # Draw the bounding box if it passes the color analysis\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, \"Fire\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "cls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1c5d2acec12d4add96bd36f840623e0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7c5a6cc488344432b2551a00a5446fff",
       "IPY_MODEL_ea44becc3eda4089b7116f5ca2e22424"
      ],
      "layout": "IPY_MODEL_9590ba52d15c442f9d694230f14eb17b"
     }
    },
    "2ddc2fe1881842928715d6bbe61579a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32f4d343d4914739b5cbd6cbc61bf921": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c5a6cc488344432b2551a00a5446fff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32f4d343d4914739b5cbd6cbc61bf921",
      "placeholder": "​",
      "style": "IPY_MODEL_d68366d0a65c47d09448cbbbd613445d",
      "value": "136.746 MB of 136.746 MB uploaded\r"
     }
    },
    "7e5fc085814840698d1ff7a7fa3641af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9590ba52d15c442f9d694230f14eb17b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d68366d0a65c47d09448cbbbd613445d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea44becc3eda4089b7116f5ca2e22424": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ddc2fe1881842928715d6bbe61579a7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e5fc085814840698d1ff7a7fa3641af",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
