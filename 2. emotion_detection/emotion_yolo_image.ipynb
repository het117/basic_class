{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from approach.ResEmoteNet import ResEmoteNet\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Settings for text\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 0.7\n",
    "font_color = (0, 0, 255)  # BGR format\n",
    "thickness = 2\n",
    "line_type = cv2.LINE_AA\n",
    "\n",
    "# Emotions labels\n",
    "emotions = ['happy', 'surprise', 'sad', 'anger', 'disgust', 'fear', 'neutral']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the YOLOv8 model for face detection\n",
    "face_detecter = YOLO(\"/root/Project/class/Emotion_detection/yolov11m-face.pt\")\n",
    "face_detecter = face_detecter.to(device)\n",
    "\n",
    "# Load the emotion classification model\n",
    "emotion_classifier = ResEmoteNet().to(device)\n",
    "checkpoint = torch.load('/root/Project/class/Emotion_detection/result/classification2024-11-18_12-38-29/best_model.pth', weights_only=True)\n",
    "emotion_classifier.load_state_dict(checkpoint)\n",
    "emotion_classifier.eval()\n",
    "\n",
    "# Image transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Emotion detection function\n",
    "def detect_emotion(image):\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = emotion_classifier(img_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    scores = probabilities.cpu().numpy().flatten()\n",
    "    rounded_scores = [round(score, 2) for score in scores]\n",
    "    return rounded_scores\n",
    "\n",
    "# Get the emotion with the highest probability\n",
    "def get_max_emotion(x, y, w, h, image):\n",
    "    crop_img = image[y:y + h, x:x + w]\n",
    "    pil_crop_img = Image.fromarray(crop_img)\n",
    "    rounded_scores = detect_emotion(pil_crop_img)\n",
    "    max_index = np.argmax(rounded_scores)\n",
    "    max_emotion = emotions[max_index]\n",
    "    return max_emotion\n",
    "\n",
    "# Display the max emotion on the image\n",
    "def print_max_emotion(x, y, image, max_emotion):\n",
    "    org = (x, y - 15)\n",
    "    cv2.putText(image, max_emotion, org, font, font_scale, font_color, thickness, line_type)\n",
    "\n",
    "# Display all emotion scores on the image\n",
    "def print_all_emotion(x, y, w, h, image):\n",
    "    crop_img = image[y:y + h, x:x + w]\n",
    "    pil_crop_img = Image.fromarray(crop_img)\n",
    "    rounded_scores = detect_emotion(pil_crop_img)\n",
    "    org = (x + w + 10, y - 20)\n",
    "    for index, value in enumerate(emotions):\n",
    "        emotion_str = f'{value}: {rounded_scores[index]:.2f}'\n",
    "        y = org[1] + 30\n",
    "        org = (org[0], y)\n",
    "        cv2.putText(image, emotion_str, org, font, font_scale, font_color, thickness, line_type)\n",
    "\n",
    "# Detect faces and emotions using YOLO\n",
    "def detect_bounding_box(image):\n",
    "    # YOLO face detection\n",
    "    results = face_detecter(image)\n",
    "    faces = []\n",
    "    \n",
    "    # Extract bounding boxes from YOLO results\n",
    "    for result in results:\n",
    "        for box in result.boxes.xyxy:  # Bounding box coordinates\n",
    "            x1, y1, x2, y2 = box[:4].int().cpu().numpy()\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            faces.append((x1, y1, w, h))\n",
    "            \n",
    "    print(faces)\n",
    "    \n",
    "    # Process each detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Emotion detection and display\n",
    "        max_emotion = get_max_emotion(x, y, w, h, image)\n",
    "        print_max_emotion(x, y, image, max_emotion)\n",
    "        print_all_emotion(x, y, w, h, image)\n",
    "    \n",
    "    return faces\n",
    "\n",
    "# Load the image file\n",
    "image = cv2.imread('/root/Project/class/emotion_detection1/video/haa.jpg')\n",
    "\n",
    "# Process the image\n",
    "faces = detect_bounding_box(image)\n",
    "\n",
    "# Save the processed image\n",
    "output_path = \"./output/ResEmoteNet_Image_Test.jpg\"\n",
    "cv2.imwrite(output_path, image)\n",
    "print(f\"Processed image saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
